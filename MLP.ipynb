{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOs5NAzzkXcowUBaR7iOaiS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raghav1378/Deep-Learning/blob/main/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Layer Perceptron (MLP) Theory\n",
        "\n",
        "A Multi-Layer Perceptron (MLP) is a type of artificial neural network characterized by its feedforward architecture and multiple layers of interconnected nodes. It's a fundamental building block in deep learning.\n",
        "\n",
        "## Key Theoretical Points:\n",
        "\n",
        "### 1. Architecture:\n",
        "* **Input Layer:** The first layer that receives the raw input data. The number of nodes here corresponds to the number of features in your dataset.\n",
        "* **Hidden Layers:** One or more layers situated between the input and output layers. These layers are crucial for learning complex, non-linear relationships within the data. The number of hidden layers (depth) and the number of neurons in each hidden layer (width) are hyperparameters.\n",
        "* **Output Layer:** The final layer of the network, responsible for producing the model's prediction. The number of nodes in this layer depends on the task:\n",
        "    * **Regression:** Typically one node for predicting a continuous value.\n",
        "    * **Binary Classification:** One node (often with a sigmoid activation for probability).\n",
        "    * **Multi-class Classification:** One node per class (often with a softmax activation for class probabilities).\n",
        "\n",
        "### 2. Neurons (Nodes):\n",
        "Each neuron in a hidden or output layer performs two main operations:\n",
        "* **Weighted Sum:** It calculates a weighted sum of its inputs from the previous layer, adding a bias term.\n",
        "    * `Net Input (z) = Σ (weight_i * input_i) + bias`\n",
        "* **Activation:** It then passes this weighted sum through a non-linear activation function.\n",
        "    * `Output (a) = Activation_Function(z)`\n",
        "\n",
        "### 3. Weights and Biases:\n",
        "* **Weights ($W$):** Numerical values that represent the strength of the connection between neurons in adjacent layers. They determine the importance of each input to a neuron.\n",
        "* **Biases ($b$):** Additional numerical values added to the weighted sum of inputs. They allow the activation function to be shifted, enabling the network to learn more complex patterns and adjust the output independent of the input values.\n",
        "* Both weights and biases are the **learnable parameters** of the MLP, meaning they are adjusted during the training process.\n",
        "\n",
        "### 4. Activation Functions:\n",
        "These functions introduce non-linearity into the network, enabling MLPs to learn and approximate complex, non-linear relationships in the data. Without them, an MLP, regardless of its depth, would effectively be just a single linear transformation.\n",
        "* **ReLU (Rectified Linear Unit):** `f(x) = max(0, x)` - Popular in hidden layers for its computational efficiency and ability to mitigate vanishing gradients.\n",
        "* **Sigmoid:** `f(x) = 1 / (1 + e^(-x))` - Outputs values between 0 and 1, often used in the output layer for binary classification.\n",
        "* **Tanh (Hyperbolic Tangent):** `f(x) = (e^x - e^(-x)) / (e^x + e^(-x))` - Outputs values between -1 and 1, similar to sigmoid but zero-centered.\n",
        "* **Softmax:** `f(x_i) = e^(x_i) / Σ e^(x_j)` - Used in the output layer for multi-class classification, converting raw scores into a probability distribution where the sum of probabilities is 1.\n",
        "\n",
        "### 5. Forward Propagation:\n",
        "This is the process of feeding the input data through the network, from the input layer, through all hidden layers, to the output layer, to generate a prediction. It's a series of matrix multiplications and activation function applications.\n",
        "\n",
        "### 6. Loss Function (Cost Function):\n",
        "A mathematical function that quantifies the difference or \"error\" between the network's predicted output and the actual target values. The primary goal of training is to minimize this loss.\n",
        "* **Mean Squared Error (MSE):** Common for regression problems.\n",
        "* **Binary Cross-Entropy:** Used for binary classification.\n",
        "* **Categorical Cross-Entropy:** Used for multi-class classification.\n",
        "\n",
        "### 7. Backpropagation:\n",
        "The fundamental algorithm for training MLPs. It's an efficient method to compute the gradients of the loss function with respect to each weight and bias in the network.\n",
        "* It works by propagating the error backwards from the output layer to the input layer, using the chain rule of calculus to determine how much each weight and bias contributed to the overall error.\n",
        "* These gradients indicate the direction and magnitude by which the parameters should be adjusted to reduce the loss.\n",
        "\n",
        "### 8. Optimization Algorithm (e.g., Gradient Descent):\n",
        "Once gradients are computed via backpropagation, an optimizer uses them to update the network's weights and biases.\n",
        "* **Gradient Descent:** Iteratively adjusts parameters in the direction opposite to the gradient of the loss function.\n",
        "* **Learning Rate:** A crucial hyperparameter that controls the step size of these updates. A small learning rate leads to slow convergence but potentially better accuracy; a large learning rate can cause overshooting and divergence.\n",
        "* Common variants: Stochastic Gradient Descent (SGD), Adam, RMSprop, Adagrad.\n",
        "\n",
        "### 9. Training Process:\n",
        "The iterative procedure to teach the MLP to perform a task:\n",
        "1.  **Initialization:** Weights and biases are typically initialized randomly (e.g., with small random values or specific techniques like Xavier/He initialization).\n",
        "2.  **Iterative Loop (Epochs):** The entire dataset is passed through the network multiple times (epochs).\n",
        "    * **Forward Pass:** Input data goes through the network, and a prediction is made.\n",
        "    * **Loss Calculation:** The loss function quantifies the error between prediction and actual target.\n",
        "    * **Backward Pass (Backpropagation):** Gradients of the loss with respect to all weights and biases are calculated.\n",
        "    * **Parameter Update:** An optimizer uses the calculated gradients and the learning rate to adjust weights and biases.\n",
        "\n",
        "### 10. Hyperparameters:\n",
        "These are parameters whose values are set **before** the training process begins, rather than being learned by the model. Their selection significantly impacts model performance.\n",
        "* Number of hidden layers\n",
        "* Number of neurons per hidden layer\n",
        "* Choice of activation functions\n",
        "* Learning rate\n",
        "* Batch size (number of samples processed before updating weights)\n",
        "* Number of epochs\n",
        "* Regularization strength (e.g., L1, L2 for `alpha` in `sklearn`)"
      ],
      "metadata": {
        "id": "XsVgCku8zb8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import make_classification # To create a synthetic dataset\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. Generate Synthetic Data\n",
        "# -----------------------------------------------------------------------------\n",
        "# We'll create a simple binary classification dataset.\n",
        "# X: features, y: target labels\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
        "                           n_redundant=5, n_classes=2, random_state=42)\n",
        "\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. Split Data into Training and Testing Sets\n",
        "# -----------------------------------------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nTraining data shape (X_train, y_train):\", X_train.shape, y_train.shape)\n",
        "print(\"Testing data shape (X_test, y_test):\", X_test.shape, y_test.shape)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. Initialize and Train the MLP Classifier\n",
        "# -----------------------------------------------------------------------------\n",
        "# MLPClassifier parameters:\n",
        "#   hidden_layer_sizes: Tuple, i-th element represents the number of neurons in the i-th hidden layer.\n",
        "#                       (e.g., (100,) means one hidden layer with 100 neurons)\n",
        "#                       (e.g., (50, 20) means two hidden layers with 50 and 20 neurons respectively)\n",
        "#   activation: Activation function for the hidden layer. ('relu', 'tanh', 'logistic', 'identity')\n",
        "#   solver: The algorithm for weight optimization. ('adam', 'lbfgs', 'sgd')\n",
        "#   alpha: L2 regularization term parameter.\n",
        "#   learning_rate_init: The initial learning rate used.\n",
        "#   max_iter: Maximum number of iterations (epochs).\n",
        "#   random_state: Seed for reproducibility.\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), # Two hidden layers: 100 neurons then 50 neurons\n",
        "                    activation='relu',             # ReLU activation for hidden layers\n",
        "                    solver='adam',                 # Adam optimizer\n",
        "                    alpha=0.0001,                  # L2 regularization\n",
        "                    batch_size='auto',             # Automatically determines batch size\n",
        "                    learning_rate_init=0.001,      # Initial learning rate\n",
        "                    max_iter=500,                  # Maximum number of epochs\n",
        "                    verbose=True,                  # Print progress messages to stdout\n",
        "                    random_state=42)\n",
        "\n",
        "print(\"\\nTraining the MLP Classifier...\")\n",
        "mlp.fit(X_train, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. Make Predictions and Evaluate the Model\n",
        "# -----------------------------------------------------------------------------\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. Inspecting Model Parameters (Optional)\n",
        "# -----------------------------------------------------------------------------\n",
        "# print(\"\\nNumber of layers:\", mlp.n_layers_)\n",
        "# print(\"Output layer activation:\", mlp.out_activation_)\n",
        "# print(\"Number of iterations (epochs):\", mlp.n_iter_)\n",
        "# print(\"Loss at the end of training:\", mlp.loss_)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dLk4Kb9Ezrys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nNumber of layers:\", mlp.n_layers_)\n",
        "print(\"Output layer activation:\", mlp.out_activation_)\n",
        "print(\"Number of iterations (epochs):\", mlp.n_iter_)\n",
        "print(\"Loss at the end of training:\", mlp.loss_)"
      ],
      "metadata": {
        "id": "7MblG8S8ztik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EVVbiNqA0Hek"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}